\section{Joint NER and EL in Newspaper Articles}
\label{sec:nel}

Newspaper articles contain a lot of current information about business relations. E.g., a German article might contain a sentence like: "`Bosch liefert Servomotoren an die DB."' (English: "`Bosch delivers servo motors to the DB."') For a human it is obvious that this sentence represents a delivery relation between the entities: "`Robert Bosch GmbH"' and "`Deutsche Bahn AG"'. While "`DB"' could also abbreviate "`Deutsche Bank"' [todo: AG?], the technical context makes clear that the railroad company (?) is meant instead of the bank.

As such relations are useful for our business graph, we want to extract those from free text, such as newspaper articles. In order to do so, we need to perform joint NER and NEL beforehand. This section describes how we identify which businesses are mentioned in free texts.\\

\subsection{Newspaper data sources}
Currently, we perform joint NER and NEL on the German Wikipedia and the German newspaper Spiegel Online [todo: link]. As Wikipedia already contains links to named entities, we use it as training data (see Sec.~\ref{sec:pipeline}) and ground truth for the evaluation [todo: reference jan]. Newspaper articles are missing such annotations.

For Spiegel Online, we use a dump that consists of HTML, contains 260000 articles (?) and has a total size of (?) GB. On (?) average, the text of each article consists of (?) characters.\\

\subsection{Process}
The joint NER and NEL consists of multiple steps, as depicted in Fig.~\ref{fig:ner_el_process}. Apart from the data import, the steps are independent from the data source so that they can be reused easily.

\begin{figure}[ht]
	\centering
  \includegraphics[width=\textwidth]{Graphics/pipeline.pdf}
	\caption{Steps of the joint NER and EL process}
	\label{fig:ner_el_process}
\end{figure}

\subsubsection{Data import}
For each article, the Spiegel Online dump provides the title, the text and a unique internal ID. We extract these three values from the dump and store them in a Cassandra table. As the German Wikipedia dump provides similar information, we store it in a table with the exact same format. The only difference to Spiegel Online is that the title of an article already is a unique ID.\\

\subsubsection{Potential named entity recognition}
A mention of a named entity in a text is a chain of one or more consecutive tokens, where a token is a semantic element of a text. This is a word in most cases, but can also be a special character, such as a comma. The task of named entity recognition is to identify such chains in text. This step is important for our process, as NEL needs to be performed on a pre-selection of those. Otherwise it would have to try to link each n-gram in the text to a named entity, which would lead to a computational complexity that increases exponential with the number of tokens. This would be inacceptable for our task.

[todo: Example with comma]

But in contrast to NER, we create a larger pre-selection of potential named entities that may signify a named entity depending on the context. This step is therefore called "`potential named entity recognition"'. It is much easier than NER, as NEL will decide to which named entity a named entity candidate links, which is a very similar task to deciding whether it links to a named entity at all. In our case, this is what NEL will do later on.

We find candidates of named entities simply by selecting all those token chains that were named entities of organizations within the training data (see Sec.~\ref{sec:alias_analysis}). An efficient way to match those is by means of a trie, which we implemented as a token based prefix tree.\\

[todo: explain logarithmic time]


\subsubsection{Feature extraction}
Before we can perform NEL on the link candidates, we need to extract features from them that are characteristic for links to specific targets. Inspired by the CohEEL project~\cite{coheel}, we decided to use three features: The link score, the entity score and the context score. Section~\ref{sec:features} describes them in detail.

We extract all these features for each link candidate. As we have a lot of explicit link annotations [todo: number] in Wikipedia, we also extracted these features from those links as well as from occurrences of organization aliases that do not signify an organization so that we have comparable labeled data.\\


\subsubsection{NEL}
By means of the labeled composite features and new ones extracted for each link candidate, we can reduce the task of NEL to a classification problem: By training a classifier model with the ground truth data it can decide whether each link candidate links to an organization or not and if it does, to which. Ehm√ºller~\cite{ehmueller} describes how the classifier works in detail and which configurations leads to the best results.

Now we know where and which organizations are mentioned in a German newspaper article. Where a text mentions multiple organizations, it is likely that it describes a relation between those. Schneider~\cite{schneider} researched how we can extract those relations in order to enrich the \textit{German Corporate Graph}.\\


\subsubsection{HTML Export}
If the user of our generated \textit{German Corporate Graph} finds a business relation that is unexpected, or if they simply want to comprehend how it was retrieved, it is necessary to provide a reference to the respective part of the original newspaper article, if applicable. An additional visualization of how the relation was extracted makes this even more useful. In terms of the joint NER and NEL process, we show where we have linked which tokens to which organizations. Therefore we enrich the original texts with HTML-links from the tokens to the Wikipedia article of their respective entity. We then export the HTML article to our Curation interface~\cite{gruner}, where the user can explore the results of the joint NER and NEL process.\\
